<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Neural Networks, Manifolds, and Topology -- colah's blog</title>
        
        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../comments/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">
        
        <link href="../../favicon.ico" rel="shortcut icon" />

        <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- start Mixpanel --><script type="text/javascript">(function(e,b){if(!b.__SV){var a,f,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
        for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=e.createElement("script");a.type="text/javascript";a.async=!0;a.src=("https:"===e.location.protocol?"https:":"http:")+'//cdn.mxpnl.com/libs/mixpanel-2.2.min.js';f=e.getElementsByTagName("script")[0];f.parentNode.insertBefore(a,f)}})(document,window.mixpanel||[]);
        mixpanel.init("dc1ac55f121e696b2f8d54d338ec642c");</script><!-- end Mixpanel -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-49811703-1', 'colah.github.io');
  ga('require', 'linkid', 'linkid.js');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>
    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" style="font-size:20px;">colah's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>

            
            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>Neural Networks, Manifolds, and Topology</h1>
                            <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on April  6, 2014</p>
    topology, neural networks, deep learning, manifold hypothesis
    <!--
        by colah
    -->
</div>
</br>

<p>Recently, there’s been a great deal of excitement and interest in deep neural networks because they’ve achieved breakthrough results in areas such as computer vision.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>However, there remain a number of concerns about them. One is that it can be quite challenging to understand <em>what</em> a neural network is really doing. If one trains it well, it achieves high quality results, but it is challenging to understand how it is doing so. If the network fails, it is hard to understand what went wrong.</p>
<p>While it is challenging to understand the behavior of deep neural networks in general, it turns out to be much easier to explore low-dimensional deep neural networks – networks that only have a few neurons in each layer. In fact, we can create visualizations to completely understand the behavior and training of such networks. This perspective will allow us to gain deeper intuition about the behavior of neural networks and observe a connection linking neural networks to an area of mathematics called topology.</p>
<p>A number of interesting things follow from this, including fundamental lower-bounds on the complexity of a neural network capable of classifying certain datasets.</p>
<h2 id="a-simple-example">A Simple Example</h2>
<p>Let’s begin with a very simple dataset, two curves on a plane. The network will learn to classify points as belonging to one or the other.</p>
<div class="centerimgcontainer">
<img src="img/simple2_data.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>The obvious way to visualize the behavior of a neural network – or any classification algorithm, for that matter – is to simply look at how it classifies every possible data point.</p>
<p>We’ll start with the simplest possible class of neural network, one with only an input layer and an output layer. Such a network simply tries to separate the two classes of data by dividing them with a line.</p>
<div class="centerimgcontainer">
<img src="img/simple2_linear.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>That sort of network isn’t very interesting. Modern neural networks generally have multiple layers between their input and output, called “hidden” layers. At the very least, they have one.</p>
<div class="centerimgcontainer">
<img src="img/example_network.svg" alt style>
<div class="caption">
Diagram of a simple network from Wikipedia
</div>
</div>
<div class="spaceafterimg">

</div>
<p>As before, we can visualize the behavior of this network by looking at what it does to different points in its domain. It separates the data with a more complicated curve than a line.</p>
<div class="centerimgcontainer">
<img src="img/simple2_0.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>With each layer, the network transforms the data, creating a new <em>representation</em>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> We can look at the data in each of these representations and how the network classifies them. When we get to the final representation, the network will just draw a line through the data (or, in higher dimensions, a hyperplane).</p>
<p>In the previous visualization, we looked at the data in its “raw” representation. You can think of that as us looking at the input layer. Now we will look at it after it is transformed by the first layer. You can think of this as us looking at the hidden layer.</p>
<p>Each dimension corresponds to the firing of a neuron in the layer.</p>
<div class="centerimgcontainer">
<img src="img/simple2_1.png" alt style>
<div class="caption">
The hidden layer learns a representation so that the data is linearly separable
</div>
</div>
<div class="spaceafterimg">

</div>
<h2 id="continuous-visualization-of-layers">Continuous Visualization of Layers</h2>
<p>In the approach outlined in the previous section, we learn to understand networks by looking at the representation corresponding to each layer. This gives us a discrete list of representations.</p>
<p>The tricky part is in understanding how we go from one to another. Thankfully, neural network layers have nice properties that make this very easy.</p>
<p>There are a variety of different kinds of layers used in neural networks. We will talk about tanh layers for a concrete example. A tanh layer <span class="math">\(\tanh(Wx+b)\)</span> consists of:</p>
<ol type="1">
<li>A linear transformation by the “weight” matrix <span class="math">\(W\)</span></li>
<li>A translation by the vector <span class="math">\(b\)</span></li>
<li>Point-wise application of tanh.</li>
</ol>
<p>We can visualize this as a continuous transformation, as follows:</p>
<div class="centerimgcontainer">
<img src="img/1layer.gif" alt="Gradually applying a neural network layer" style>
</div>
<div class="spaceafterimg">

</div>
<p>The story is much the same for other standard layers, consisting of an affine transformation followed by pointwise application of a monotone activation function.</p>
<p>We can apply this technique to understand more complicated networks. For example, the following network classifies two spirals that are slightly entangled, using four hidden layers. Over time, we can see it shift from the “raw” representation to higher level ones it has learned in order to classify the data. While the spirals are originally entangled, by the end they are linearly separable.</p>
<div class="centerimgcontainer">
<img src="img/spiral.1-2.2-2-2-2-2-2.gif" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>On the other hand, the following network, also using multiple layers, fails to classify two spirals that are more entangled.</p>
<div class="centerimgcontainer">
<img src="img/spiral.2.2-2-2-2-2-2-2.gif" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>It is worth explicitly noting here that these tasks are only somewhat challenging because we are using low-dimensional neural networks. If we were using wider networks, all this would be quite easy.</p>
<p><em>(Andrej Karpathy has made a <a href="http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html">nice demo</a> based on ConvnetJS that allows you to interactively explore networks with this sort of visualization of training!)</em></p>
<h2 id="topology-of-tanh-layers">Topology of tanh Layers</h2>
<p>Each layer stretches and squishes space, but it never cuts, breaks, or folds it. Intuitively, we can see that it preserves topological properties. For example, a set will be connected afterwards if it was before (and vice versa).</p>
<p>Transformations like this, which don’t affect topology, are called homeomorphisms. Formally, they are bijections that are continuous functions both ways.</p>
<p><strong>Theorem</strong>: Layers with <span class="math">\(N\)</span> inputs and <span class="math">\(N\)</span> outputs are homeomorphisms, if the weight matrix, <span class="math">\(W\)</span>, is non-singular. (Though one needs to be careful about domain and range.)</p>
<p><strong>Proof</strong>: Let’s consider this step by step:</p>
<ol type="1">
<li>Let’s assume <span class="math">\(W\)</span> has a non-zero determinant. Then it is a bijective linear function with a linear inverse. Linear functions are continuous. So, multiplying by <span class="math">\(W\)</span> is a homeomorphism.</li>
<li>Translations are homeomorphisms</li>
<li>tanh (and sigmoid and softplus but not ReLU) are continuous functions with continuous inverses. They are bijections if we are careful about the domain and range we consider. Applying them pointwise is a homeomorphism</li>
</ol>
<p>Thus, if <span class="math">\(W\)</span> has a non-zero determinant, our layer is a homeomorphism. ∎</p>
<p>This result continues to hold if we compose arbitrarily many of these layers together.</p>
<h2 id="topology-and-classification">Topology and Classification</h2>
<div class="floatrightimgcontainer">
<img src="img/topology_base.png" alt style>
<div class="caption">
<span class="math">\(A\)</span> is red, <span class="math">\(B\)</span> is blue
</div>
</div>
<div class="spaceafterimg">

</div>
<p>Consider a two dimensional dataset with two classes <span class="math">\(A, B \subset \mathbb{R}^2\)</span>:</p>
<p><span class="math">\[A = \{x | d(x,0) &lt; 1/3\}\]</span></p>
<p><span class="math">\[B = \{x | 2/3 &lt; d(x,0) &lt; 1\}\]</span></p>
<p><strong>Claim</strong>: It is impossible for a neural network to classify this dataset without having a layer that has 3 or more hidden units, regardless of depth.</p>
<p>As mentioned previously, classification with a sigmoid unit or a softmax layer is equivalent to trying to find a hyperplane (or in this case a line) that separates <span class="math">\(A\)</span> and <span class="math">\(B\)</span> in the final represenation. With only two hidden units, a network is topologically incapable of separating the data in this way, and doomed to failure on this dataset.</p>
<p>In the following visualization, we observe a hidden representation while a network trains, along with the classification line. As we watch, it struggles and flounders trying to learn a way to do this.</p>
<div class="centerimgcontainer">
<img src="img/topology_2D-2D_train.gif" alt style>
<div class="caption">
For this network, hard work isn’t enough.
</div>
</div>
<div class="spaceafterimg">

</div>
<p>In the end it gets pulled into a rather unproductive local minimum. Although, it’s actually able to achieve <span class="math">\(\sim 80\%\)</span> classification accuracy.</p>
<p>This example only had one hidden layer, but it would fail regardless.</p>
<p><strong>Proof</strong>: Either each layer is a homeomorphism, or the layer’s weight matrix has determinant 0. If it is a homemorphism, <span class="math">\(A\)</span> is still surrounded by <span class="math">\(B\)</span>, and a line can’t separate them. But suppose it has a determinant of 0: then the dataset gets collapsed on some axis. Since we’re dealing with something homeomorphic to the original dataset, <span class="math">\(A\)</span> is surrounded by <span class="math">\(B\)</span>, and collapsing on any axis means we will have some points of <span class="math">\(A\)</span> and <span class="math">\(B\)</span> mix and become impossible to distinguish between. ∎</p>
<p>If we add a third hidden unit, the problem becomes trivial. The neural network learns the following representation:</p>
<div class="centerimgcontainer">
<img src="img/topology_3d.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>With this representation, we can separate the datasets with a hyperplane.</p>
<p>To get a better sense of what’s going on, let’s consider an even simpler dataset that’s 1-dimensional:</p>
<div class="floatrightimgcontainer">
<img src="img/topology_1d.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p><span class="math">\[A = [-\frac{1}{3}, \frac{1}{3}]\]</span></p>
<p><span class="math">\[B = [-1, -\frac{2}{3}] \cup [\frac{2}{3}, 1]\]</span></p>
<p>Without using a layer of two or more hidden units, we can’t classify this dataset. But if we use one with two units, we learn to represent the data as a nice curve that allows us to separate the classes with a line:</p>
<div class="centerimgcontainer">
<img src="img/topology_1D-2D_train.gif" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>What’s happening? One hidden unit learns to fire when <span class="math">\(x &gt; -\frac{1}{2}\)</span> and one learns to fire when <span class="math">\(x &gt; \frac{1}{2}\)</span>. When the first one fires, but not the second, we know that we are in A.</p>
<h2 id="the-manifold-hypothesis">The Manifold Hypothesis</h2>
<p>Is this relevant to real world data sets, like image data? If you take the manifold hypothesis really seriously, I think it bears consideration.</p>
<p>The manifold hypothesis is that natural data forms lower-dimensional manifolds in its embedding space. There are both theoretical<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> and experimental<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> reasons to believe this to be true. If you believe this, then the task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds.</p>
<p>In the previous examples, one class completely surrounded another. However, it doesn’t seem very likely that the dog image manifold is completely surrounded by the cat image manifold. But there are other, more plausible topological situations that could still pose an issue, as we will see in the next section.</p>
<h2 id="links-and-homotopy">Links And Homotopy</h2>
<p>Another interesting dataset to consider is two linked tori, <span class="math">\(A\)</span> and <span class="math">\(B\)</span>.</p>
<div class="centerimgcontainer">
<img src="img/link.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>Much like the previous datasets we considered, this dataset can’t be separated without using <span class="math">\(n+1\)</span> dimensions, namely a <span class="math">\(4\)</span>th dimension.</p>
<p>Links are studied in knot theory, an area of topology. Sometimes when we see a link, it isn’t immediately obvious whether it’s an unlink (a bunch of things that are tangled together, but can be separated by continuous deformation) or not.</p>
<div class="bigcenterimgcontainer">
<img src="img/unlink-2spiral.png" alt style>
<div class="caption">
A relatively simple unlink.
</div>
</div>
<div class="spaceafterimg">

</div>
<p>If a neural network using layers with only 3 units can classify it, then it is an unlink. (Question: Can all unlinks be classified by a network with only 3 units, theoretically?)</p>
<p>From this knot perspective, our continuous visualization of the representations produced by a neural network isn’t just a nice animation, it’s a procedure for untangling links. In topology, we would call it an <em>ambient isotopy</em> between the original link and the separated ones.</p>
<p>Formally, an ambient isotopy between manifolds <span class="math">\(A\)</span> and <span class="math">\(B\)</span> is a continuous function <span class="math">\(F: [0,1] \times X \to Y\)</span> such that each <span class="math">\(F_t\)</span> is a homeomorphism from <span class="math">\(X\)</span> to its range, <span class="math">\(F_0\)</span> is the identity function, and <span class="math">\(F_1\)</span> maps <span class="math">\(A\)</span> to <span class="math">\(B\)</span>. That is, <span class="math">\(F_t\)</span> continuously transitions from mapping <span class="math">\(A\)</span> to itself to mapping <span class="math">\(A\)</span> to <span class="math">\(B\)</span>.</p>
<p><strong>Theorem</strong>: There is an ambient isotopy between the input and a network layer’s representation if: a) <span class="math">\(W\)</span> isn’t singular, b) we are willing to permute the neurons in the hidden layer, and c) there is more than 1 hidden unit.</p>
<p><strong>Proof</strong>: Again, we consider each stage of the network individually:</p>
<ol type="1">
<li>The hardest part is the linear transformation. In order for this to be possible, we need <span class="math">\(W\)</span> to have a positive determinant. Our premise is that it isn’t zero, and we can flip the sign if it is negative by switching two of the hidden neurons, and so we can guarantee the determinant is positive. The space of positive determinant matrices is <a href="http://en.wikipedia.org/wiki/Connected_space#Path_connectedness">path-connected</a>, so there exists <span class="math">\(p: [0,1] \to GL_n(\mathbb{R})\)</span><a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> such that <span class="math">\(p(0) = Id\)</span> and <span class="math">\(p(1) = W\)</span>. We can continually transition from the identity function to the <span class="math">\(W\)</span> transformation with the function <span class="math">\(x \to p(t)x\)</span>, multiplying <span class="math">\(x\)</span> at each point in time <span class="math">\(t\)</span> by the continuously transitioning matrix <span class="math">\(p(t)\)</span>.</li>
</ol>
<ol start="2" type="1">
<li>We can continually transition from the identity function to the <span class="math">\(b\)</span> translation with the function <span class="math">\(x \to x + tb\)</span>.</li>
<li>We can continually transition from the identity function to the pointwise use of σ with the function: <span class="math">\(x \to (1-t)x + tσ(x)\)</span>. ∎</li>
</ol>
<p>I imagine there is probably interest in programs automatically discovering such ambient isotopies and automatically proving the equivalence of certain links, or that certain links are separable. It would be interesting to know if neural networks can beat whatever the state of the art is there.</p>
<p><em>(Apparently determining if knots are trivial is NP. This doesn’t bode well for neural networks.)</em></p>
<p>The sort of links we’ve talked about so far don’t seem likely to turn up in real world data, but there are higher dimensional generalizations. It seems plausible such things could exist in real world data.</p>
<p>Links and knots are <span class="math">\(1\)</span>-dimensional manifolds, but we need 4 dimensions to be able to untangle all of them. Similarly, one can need yet higher dimensional space to be able to unknot <span class="math">\(n\)</span>-dimensional manifolds. All <span class="math">\(n\)</span>-dimensional manifolds can be untangled in <span class="math">\(2n+2\)</span> dimensions.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p><em>(I know very little about knot theory and really need to learn more about what’s known regarding dimensionality and links. If we know a manifold can be embedded in n-dimensional space, instead of the dimensionality of the manifold, what limit do we have?)</em></p>
<h2 id="the-easy-way-out">The Easy Way Out</h2>
<p>The natural thing for a neural net to do, the very easy route, is to try and pull the manifolds apart naively and stretch the parts that are tangled as thin as possible. While this won’t be anywhere close to a genuine solution, it can achieve relatively high classification accuracy and be a tempting local minimum.</p>
<div class="bigcenterimgcontainer">
<img src="img/tangle.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>It would present itself as very high derivatives on the regions it is trying to stretch, and sharp near-discontinuities. We know these things happen.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> Contractive penalties, penalizing the derivatives of the layers at data points, are the natural way to fight this.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>Since these sort of local minima are absolutely useless from the perspective of trying to solve topological problems, topological problems may provide a nice motivation to explore fighting these issues.</p>
<p>On the other hand, if we only care about achieving good classification results, it seems like we might not care. If a tiny bit of the data manifold is snagged on another manifold, is that a problem for us? It seems like we should be able to get arbitrarily good classification results despite this issue.</p>
<p><em>(My intuition is that trying to cheat the problem like this is a bad idea: it’s hard to imagine that it won’t be a dead end. In particular, in an optimization problem where local minima are a big problem, picking an architecture that can’t genuinely solve the problem seems like a recipe for bad performance.)</em></p>
<h2 id="better-layers-for-manipulating-manifolds">Better Layers for Manipulating Manifolds?</h2>
<p>The more I think about standard neural network layers – that is, with an affine transformation followed by a point-wise activation function – the more disenchanted I feel. It’s hard to imagine that these are really very good for manipulating manifolds.</p>
<p>Perhaps it might make sense to have a very different kind of layer that we can use in composition with more traditional ones?</p>
<p>The thing that feels natural to me is to learn a vector field with the direction we want to shift the manifold:</p>
<div class="centerimgcontainer">
<img src="img/grid_vec.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>And then deform space based on it:</p>
<div class="centerimgcontainer">
<img src="img/grid_bubble.png" alt style>
</div>
<div class="spaceafterimg">

</div>
<p>One could learn the vector field at fixed points (just take some fixed points from the training set to use as anchors) and interpolate in some manner. The vector field above is of the form:</p>
<p><span class="math">\[F(x) = \frac{v_0f_0(x) + v_1f_1(x)}{1+f_0(x)+f_1(x)}\]</span></p>
<p>Where <span class="math">\(v_0\)</span> and <span class="math">\(v_1\)</span> are vectors and <span class="math">\(f_0(x)\)</span> and <span class="math">\(f_1(x)\)</span> are n-dimensional gaussians. This is inspired a bit by <a href="http://en.wikipedia.org/wiki/Radial_basis_function">radial basis functions</a>.</p>
<h2 id="k-nearest-neighbor-layers">K-Nearest Neighbor Layers</h2>
<p>I’ve also begun to think that linear separability may be a huge, and possibly unreasonable, amount to demand of a neural network. In some ways, it feels like the natural thing to do would be to use <a href="knn">k-nearest neighbors</a> (k-NN). However, k-NN’s success is greatly dependent on the representation it classifies data from, so one needs a good representation before k-NN can work well.</p>
<p>As a first experiment, I trained some MNIST networks (two-layer convolutional nets, no dropout) that achieved <span class="math">\(\sim 1\%\)</span> test error. I then dropped the final softmax layer and used the k-NN algorithm. I was able to consistently achieve a reduction in test error of 0.1-0.2%.</p>
<p>Still, this doesn’t quite feel like the right thing. The network is still trying to do linear classification, but since we use k-NN at test time, it’s able to recover a bit from mistakes it made.</p>
<p>k-NN is differentiable with respect to the representation it’s acting on, because of the 1/distance weighting. As such, we can train a network directly for k-NN classification. This can be thought of as a kind of “nearest neighbor” layer that acts as an alternative to softmax.</p>
<p>We don’t want to feedforward our entire training set for each mini-batch because that would be very computationally expensive. I think a nice approach is to classify each element of the mini-batch based on the classes of other elements of the mini-batch, giving each one a weight of 1/(distance from classification target).<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>Sadly, even with sophisticated architecture, using k-NN only gets down to 5-4% test error – and using simpler architectures gets worse results. However, I’ve put very little effort into playing with hyper-parameters.</p>
<p>Still, I really aesthetically like this approach, because it seems like what we’re “asking” the network to do is much more reasonable. We want points of the same manifold to be closer than points of others, as opposed to the manifolds being separable by a hyperplane. This should correspond to inflating the space between manifolds for different categories and contracting the individual manifolds. It feels like simplification.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Topological properties of data, such as links, may make it impossible to linearly separate classes using low-dimensional networks, regardless of depth. Even in cases where it is technically possible, such as spirals, it can be very challenging to do so.</p>
<p>To accurately classify data with neural networks, wide layers are sometimes necessary. Further, traditional neural network layers do not seem to be very good at representing important manipulations of manifolds; even if we were to cleverly set weights by hand, it would be challenging to compactly represent the transformations we want. New layers, specifically motivated by the manifold perspective of machine learning, may be useful supplements.</p>
<p><em>(This is a developing research project. It’s posted as an experiment in doing research openly. I would be delighted to have your feedback on these ideas: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request <a href="https://github.com/colah/NN-Topology-Post">on github</a>.)</em></p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thank you to Yoshua Bengio, Michael Nielsen, Dario Amodei, Eliana Lorch, Jacob Steinhardt, and Tamsyn Waterhouse for their comments and encouragement.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This seems to have really kicked off with <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">Krizhevsky <em>et al.</em>, (2012)</a>, who put together a lot of different pieces to achieve outstanding results. Since then there’s been a lot of other exciting work.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>These representations, hopefully, make the data “nicer” for the network to classify. There has been a lot of work exploring representations recently. Perhaps the most fascinating has been in Natural Language Processing: the representations we learn of words, called word embeddings, have interesting properties. See <a href="http://research.microsoft.com/pubs/189726/rvecs.pdf">Mikolov <em>et al.</em> (2013)</a>, <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf">Turian <em>et al.</em> (2010)</a>, and, <a href="http://www.socher.org/">Richard Socher’s work</a>. To give you a quick flavor, there is a <a href="http://metaoptimize.s3.amazonaws.com/cw-embeddings-ACL2010/embeddings-mostcommon.EMBEDDING_SIZE=50.png">very nice visualization</a> associated with the Turian paper.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>A lot of the natural transformations you might want to perform on an image, like translating or scaling an object in it, or changing the lighting, would form continuous curves in image space if you performed them continuously.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p><a href="http://comptop.stanford.edu/u/preprints/mumford.pdf">Carlsson <em>et al.</em></a> found that local patches of images form a klein bottle.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p><span class="math">\(GL_n(\mathbb{R})\)</span> is the set of invertible <span class="math">\(n \times n\)</span> matrices on the reals, formally called the <a href="http://en.wikipedia.org/wiki/General_linear_group">general linear group</a> of degree <span class="math">\(n\)</span>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>This result is mentioned in <a href="http://en.wikipedia.org/wiki/Whitney_embedding_theorem#Isotopy_versions">Wikipedia’s subsection on Isotopy versions</a>.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>See <a href="http://cs.nyu.edu/~zaremba/docs/understanding.pdf">Szegedy <em>et al.</em></a>, where they are able to modify data samples and find slight modifications that cause some of the best image classification neural networks to misclasify the data. It’s quite troubling.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Contractive penalties were introduced in contractive autoencoders. See <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf">Rifai <em>et al.</em> (2011)</a>.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>I used a slightly less elegant, but roughly equivalent algorithm because it was more practical to implement in Theano: feedforward two different batches at the same time, and classify them based on each other.<a href="#fnref9">↩</a></p></li>
</ol>
</section>

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../comments/inlineDisqussions.js"></script>
<script src="../../js/disqus.js"></script>

                        </div>
                        <div class="col-md-4"></div>
                    </div>
                </div>
            </div>
        

            <div id="footer">
                <div class="container">
                    Built by <a href="https://github.com/oinkina">Oinkina</a> with
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
                    using <a href="http://getbootstrap.com/">Bootstrap</a>, 
                    <a href="http://www.mathjax.org/">MathJax</a>,
                    <a href="http://disqus.com/">Disqus</a>,
                    <a href="https://github.com/unconed/MathBox.js">MathBox.js</a>,
                    <a href="http://highlightjs.org/">Highlight.js</a>, 
                    and <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>

    <script src="../../comments/inlineDisqussions.js"></script>
    
    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
