<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>colah's blog</title>
        <link>http://colah.github.io/</link>
        <description><![CDATA[]]></description>
        <atom:link href="http://colah.github.io//posts/tags/math.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Sun, 13 Jul 2014 00:00:00 UT</lastBuildDate>
        <item>
    <title>Understanding Convolutions</title>
    <link>http://colah.github.io/posts/2014-07-Understanding-Convolutions/</link>
    <description><![CDATA[<p>In a <a href="../2014-07-Conv-Nets-Modular/">previous post</a>, we built up an understanding of convolutional neural networks, without referring to any significant mathematics. To go further, however, we need to understand convolutions.</p>
<p>If we just wanted to understand convolutional neural networks, it might suffice to roughly understand convolutions. But the aim of this series is to bring us to the frontier of convolutional neural networks and explore new options. To do that, we’re going to need to understand convolutions very deeply.</p>
<p>Thankfully, with a few examples, convolution becomes quite a straightforward idea.</p>
<h1 id="lessons-from-a-dropped-ball">Lessons from a Dropped Ball</h1>
<p>Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. <em>How likely is it that a ball will go a distance <span class="math">\(c\)</span> if you drop it and then drop it again from above the point at which it landed?</em></p>
<p>Let’s break this down. After the first drop, it will land <span class="math">\(a\)</span> units away from the starting point with probability <span class="math">\(f(a)\)</span>, where <span class="math">\(f\)</span> is the probability distribution.</p>
<p>Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling <span class="math">\(b\)</span> units away from the new starting point is <span class="math">\(g(b)\)</span>, where <span class="math">\(g\)</span> may be a different probability distribution if it’s dropped from a different height.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-fagb.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>If we fix the result of the first drop so we know the ball went distance <span class="math">\(a\)</span>, for the ball to go a total distance <span class="math">\(c\)</span>, the distance traveled in the second drop is also fixed at <span class="math">\(b\)</span>, where <span class="math">\(a+b=c\)</span>. So the probability of this happening is simply <span class="math">\(f(a) \cdot g(b)\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Let’s think about this with a specific discrete example. We want the total distance <span class="math">\(c\)</span> to be 3. If the first time it rolls, <span class="math">\(a=2\)</span>, the second time it must roll <span class="math">\(b=1\)</span> in order to reach our total distance <span class="math">\(a+b=3\)</span>. The probability of this is <span class="math">\(f(2) \cdot g(1)\)</span>.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-split-21.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, as long as they add to 3.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-splits-12-03.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>The probabilities are <span class="math">\(f(1) \cdot g(2)\)</span> and <span class="math">\(f(0) \cdot g(3)\)</span>, respectively.</p>
<p>In order to find the <em>total likelihood</em> of the ball reaching a total distance of <span class="math">\(c\)</span>, we can’t consider only one possible way of reaching <span class="math">\(c\)</span>. Instead, we consider <em>all the possible ways</em> of partitioning <span class="math">\(c\)</span> into two drops <span class="math">\(a\)</span> and <span class="math">\(b\)</span> and sum over the <em>probability of each way</em>.</p>
<p><span class="math">\[...~~ f(0)\!\cdot\! g(3) ~+~ f(1)\!\cdot\! g(2) ~+~ f(2)\!\cdot\! g(1)~~...\]</span></p>
<p>We already know that the probability for each case of <span class="math">\(a+b=c\)</span> is simply <span class="math">\(f(a) \cdot g(b)\)</span>. So, summing over every solution to <span class="math">\(a+b=c\)</span>, we can denote the total likelihood as:</p>
<p><span class="math">\[\sum_{a+b=c} f(a) \cdot g(b)\]</span></p>
<p>Turns out, we’re doing a convolution! In particular, the convolution of <span class="math">\(f\)</span> and <span class="math">\(g\)</span>, evaluated at <span class="math">\(c\)</span> is defined:</p>
<p><span class="math">\[(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~\]</span></p>
<p>If we substitute <span class="math">\(b = c-a\)</span>, we get:</p>
<p><span class="math">\[(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)\]</span></p>
<p>This is the standard definition<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> of convolution.</p>
<p>To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position <span class="math">\(a\)</span> with probability <span class="math">\(f(a)\)</span>. If it lands at <span class="math">\(a\)</span>, it has probability <span class="math">\(g(c-a)\)</span> of landing at a position <span class="math">\(c\)</span>.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-OnePath.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>To get the convolution, we consider all intermediate positions.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-SumPaths.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<h1 id="visualizing-convolutions">Visualizing Convolutions</h1>
<p>There’s a very nice trick that helps one think about convolutions more easily.</p>
<p>First, an observation. Suppose the probability that a ball lands a certain distance <span class="math">\(x\)</span> from where it started is <span class="math">\(f(x)\)</span>. Then, afterwards, the probability given that it started a distance <span class="math">\(x\)</span> from where it landed is <span class="math">\(f(-x)\)</span>.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-Reverse.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>If we know the ball lands at a position <span class="math">\(c\)</span> after the second drop, what is the probability that the previous position was <span class="math">\(a\)</span>?</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-BackProb.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>So the probability that the previous position was <span class="math">\(a\)</span> is <span class="math">\(g(-(a-c)) = g(c-a)\)</span>.</p>
<p>Now, consider the probability each intermediate position contributes to the ball finally landing at <span class="math">\(c\)</span>. We know the probability of the first drop putting the ball into the intermediate position a is <span class="math">\(f(a)\)</span>. We also know that the probability of it having been in <span class="math">\(a\)</span>, if it lands at <span class="math">\(c\)</span> is <span class="math">\(g(c-a)\)</span>.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-Intermediate.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>Summing over the <span class="math">\(a\)</span>s, we get the convolution.</p>
<p>The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value <span class="math">\(c\)</span> in a single picture. By shifting the bottom half shifting around, we can evaluate the convolution at other values of <span class="math">\(c\)</span>. This allows us to understand the convolution as a whole.</p>
<p>For example, we can see that it peaks when the distributions align.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-Intermediate-Align.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>And shrinks as the intersection between the distributions gets smaller.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-Intermediate-Sep.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>By using this trick in an animation, it really becomes possible to visually understand convolutions.</p>
<p>Below, we’re able to visualize the convolution of two box functions:</p>
<div class="bigcenterimgcontainer">
<img src="img/Wiki-BoxConvAnim.gif" alt="" style="">
<div class="caption">
From Wikipedia
</div>
</div>
<div class="spaceafterimg">

</div>
<p>Armed with this perspective, a lot of things become more intuitive.</p>
<p>Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.</p>
<h1 id="higher-dimensional-convolutions">Higher Dimensional Convolutions</h1>
<p>Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.</p>
<p>Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.</p>
<div class="bigcenterimgcontainer">
<img src="img/ProbConv-TwoDim.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>Convolution is the same as before:</p>
<p><span class="math">\[(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)\]</span></p>
<p>Except, now <span class="math">\(a\)</span>, <span class="math">\(b\)</span> and <span class="math">\(c\)</span> are vectors. To be more explicit,</p>
<p><span class="math">\[(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)\]</span></p>
<p>Or in the standard definition:</p>
<p><span class="math">\[(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)\]</span></p>
<p>Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.</p>
<p>One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”</p>
<div class="centerimgcontainer">
<img src="img/RiverTrain-ImageConvDiagram.png" alt="" style="">
<div class="caption">
From the <a href="http://rivertrail.github.io/RiverTrail/tutorial/">River Trail documentation</a>
</div>
</div>
<div class="spaceafterimg">

</div>
<p>The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.</p>
<p>For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value <span class="math">\(1/9\)</span> on each pixel in the box,</p>
<div class="bigcenterimgcontainer">
<img src="img/Gimp-Blur.png" alt="" style="">
<div class="caption">
Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html">Gimp documentation</a>
</div>
</div>
<div class="spaceafterimg">

</div>
<p>We can also detect edges by taking the values <span class="math">\(-1\)</span> and <span class="math">\(1\)</span> on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.</p>
<div class="bigcenterimgcontainer">
<img src="img/Gimp-Edge.png" alt="" style="">
<div class="caption">
Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html">Gimp documentation</a>
</div>
</div>
<div class="spaceafterimg">

</div>
<p>The gimp documentation has <a href="http://docs.gimp.org/en/plug-in-convmatrix.html">many other examples</a>.</p>
<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>
<p>So, how does convolution relate to convolutional neural networks?</p>
<p>Consider a 1-dimensional convolutional layer with inputs <span class="math">\(\{x_n\}\)</span> and outputs <span class="math">\(\{y_n\}\)</span>, like we discussed in the <a href="../2014-07-Conv-Nets-Modular/">previous post</a>:</p>
<div class="bigcenterimgcontainer">
<img src="img/Conv-9-Conv2-XY.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>As we observed, we can describe the outputs in terms of the inputs:</p>
<p><span class="math">\[y_n = A(x_{n}, x_{n+1}, ...)\]</span></p>
<p>Generally, <span class="math">\(A\)</span> would be multiple neurons. But suppose it is a single neuron for a moment.</p>
<p>Recall that a typical neuron in a neural network is described by:</p>
<p><span class="math">\[\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~...~ + b)\]</span></p>
<p>Where <span class="math">\(x_0\)</span>, <span class="math">\(x_1\)</span>… are the inputs. The weights, <span class="math">\(w_0\)</span>, <span class="math">\(w_1\)</span>, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Saying that multiple neurons are identical is the same thing as saying that the weights are the same.</p>
<p>It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.</p>
<p>Typically, we describe all the neurons in a layers at once, rather than individually. The trick is to have a weight matrix, <span class="math">\(W\)</span>:</p>
<p><span class="math">\[y = \sigma(Wx + b)\]</span></p>
<p>Where <span class="math">\(\sigma\)</span> denotes sigmoid or another neuron activation function.</p>
<p>For example, we get:</p>
<p><span class="math">\[y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 ...)\]</span></p>
<p><span class="math">\[y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 ...)\]</span></p>
<p>Each row of the matrix describes the weights connect a neuron to its inputs.</p>
<p>Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.</p>
<div class="bigcenterimgcontainer">
<img src="img/Conv-9-Conv2-XY-W.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>Which corresponds to the equations:</p>
<p><span class="math">\[y_0 = \sigma(W_0x_0 + W_1x_1 -b)\]</span></p>
<p><span class="math">\[y_1 = \sigma(W_0x_1 + W_1x_2 -b)\]</span></p>
<p>So while, normally, a weight matrix connects every input to every neuron with different weights:</p>
<p><span class="math">\[W = \left[\begin{array}{ccccc} 
W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; ...\\
W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; ...\\
W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; ...\\
W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; ...\\
...     &amp;   ...   &amp;   ...   &amp;  ...    &amp; ...\\
\end{array}\right]\]</span></p>
<p>The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.</p>
<p><span class="math">\[W = \left[\begin{array}{ccccc} 
w_0 &amp; w_1 &amp;  0  &amp;  0  &amp; ...\\
 0  &amp; w_0 &amp; w_1 &amp;  0  &amp; ...\\
 0  &amp;  0  &amp; w_0 &amp; w_1 &amp; ...\\
 0  &amp;  0  &amp;  0  &amp; w_0 &amp; ...\\
... &amp; ... &amp; ... &amp; ... &amp; ...\\
\end{array}\right]\]</span></p>
<p>Multiplying by the above matrix is the same thing as convolving with <span class="math">\([...0, w_1, w_0, 0...]\)</span>. The function sliding to different positions corresponds to having neurons at those positions.</p>
<p>What about two-dimensional convolutional layers?</p>
<div class="centerimgcontainer">
<img src="img/Conv2-5x5-Conv2-XY.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.</p>
<p>Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.</p>
<h1 id="conclusion">Conclusion</h1>
<p>We introduced a lot of mathematical machinery in this blog post, but it may not be obvious what we gained. Convolution is obviously a useful tool in probability theory and computer graphics, but what do we gain from phrasing convolutional neural networks in terms of convolutions?</p>
<p>The first advantage is that we have some very powerful language for describing the wiring of networks. The examples we’ve dealt with so far haven’t been complicated enough for this benefit to become clear, but convolutions will allow us to get rid of huge amounts of unpleasant book-keeping for us.</p>
<p>Secondly, convolutions come with significant implementational advantages. Many libraries provide highly efficient convolution routines. Further, while convolution naively appears to be an <span class="math">\(O(n^2)\)</span> operation, using some rather deep mathematical insights, it is possible to create a <span class="math">\(O(n\log(n))\)</span> implementation. We will discuss this in much greater detail in a future post.</p>
<p>In fact, the use of highly-efficient parallel convolution implementations on GPUs has been essential to recent progress in computer vision.</p>
<h1 id="next-posts-in-this-series">Next Posts in this Series</h1>
<p>This post is part of a series on convolutional neural networks and their generalizations. The first two posts will be review for those familiar with deep learning, while later ones should be of interest to everyone. To get updates, subscribe to my <a href="../../rss.xml">RSS feed</a>!</p>
<p>Please comment below or on the side. Pull requests can be made on <a href="https://github.com/colah/colah.github.io">github</a>.</p>
<h1 id="acknowledgments">Acknowledgments</h1>
<p>I’m extremely grateful to Eliana Lorch, for extensive discussion of convolutions and help writing this post.</p>
<p>I’m also grateful to Michael Nielsen and Dario Amodei for their comments and support.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We want the probability of the ball rolling <span class="math">\(a\)</span> units the first time and also rolling <span class="math">\(b\)</span> units the second time. The distributions <span class="math">\(P(A) = f(a)\)</span> and <span class="math">\(P(b) = g(b)\)</span> are independent, with both distributions centered at 0. So <span class="math">\(P(a,b) = P(a) * P(b) = f(a) \cdot g(b)\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.</p>
<p>For example, convolution is a commutative operation. That is, <span class="math">\(f\ast g = g\ast f\)</span>. Why?</p>
<p><span class="math">\[\sum_{a+b=c} f(a) \cdot g(b) ~~=~  \sum_{b+a=c} g(b) \cdot f(a)\]</span></p>
<p>Convolution is also associative. That is, <span class="math">\((f\ast g)\ast h = f\ast (g\ast h)\)</span>. Why?</p>
<p><span class="math">\[\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))\]</span><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.<a href="#fnref3">↩</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sun, 13 Jul 2014 00:00:00 UT</pubDate>
    <guid>http://colah.github.io/posts/2014-07-Understanding-Convolutions/</guid>
</item>
<item>
    <title>Fanfiction, Graphs, and PageRank</title>
    <link>http://colah.github.io/posts/2014-07-FFN-Graphs-Vis/</link>
    <description><![CDATA[<p>On a website called fanfiction.net, users write millions of stories about their favorite stories. They have diverse opinions about them. They love some stories, and hate others. The opinions are noisy, and it’s hard to see the big picture.</p>
<p>With tools from mathematics and some helpful software, however, we can visualize the underlying structure.</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-HP-ships-labeled.png" alt="" style="">
<div class="caption">
Graph of Harry Potter Fanfiction, colored by ship
</div>
</div>
<div class="spaceafterimg">

</div>
<p>In the following post, we will visualize the Harry Potter, Naruto and Twilight fandoms on fanfiction.net. We will also use Google’s PageRank algorithm to rank stories, and perform collaborative filtering to make story recommendations to top fanfiction.net users.</p>
<p>If you’re not interested in the details, you can skip to the following:</p>
<p><strong>Interactive Graphs</strong>: <a href="graphs/HP-ship/">Harry Potter</a>, <a href="graphs/NAR-ship/">Naruto</a>, <a href="graphs/TWI-ship/">Twilight</a></p>
<p><strong>Story Rankings</strong>: <a href="pagerank/hp.html">Harry Potter</a>, <a href="pagerank/naruto.html">Naruto</a>, <a href="pagerank/twi.html">Twilight</a></p>
<p><strong>Story Recommendations</strong>: <a href="recs/hp.html">Harry Potter</a>, <a href="recs/nar.html">Naruto</a>, <a href="recs/twi.html">Twilight</a></p>
<p>And of course, you might skim below to see the pretty pictures!</p>
<h2 id="introduction">Introduction</h2>
<p>Fanfiction is a wide-spread phenomenon where fans of different works write derivative stories. This ranges from young children writing their first stories about their favorite fictional characters, to professional-quality stories written by aspiring novelists. Many such stories are posted to websites where they are read by a large audience and commented on. The largest such website is <a href="https://www.fanfiction.net/">fanficiton.net</a>.</p>
<p>The sheer amount of fanfiction out there is rather staggering. The total number of stories on fanfiction.net exceeds six million. Harry Potter stories account for around 14% of these, followed by Naruto (around 7%) and Twilight (around 4%) (<a href="http://ffnresearch.blogspot.com/2010/07/fanfictionnet-story-totals.html">FFN Research</a>). The majority of these stories have very little in the way of readership, but popular stories can have a large number of readers.</p>
<p>Some research was done into the demographics of fanfiction.net users and other topics by <a href="http://ffnresearch.blogspot.com/">FFN Research</a>. They found that 78% of fanfiction.net authors who joined in 2010 identified as female. Further, around 80% of users who report their age are between 13 and 17.</p>
<p>A lot of other interesting research and analysis has been done on the blogs <a href="http://destinationtoast.tumblr.com/stats">Destination: Toast!</a> and <a href="http://toastystats.tumblr.com/">TOASTYSTATS</a>.</p>
<h2 id="basic-methods">Basic Methods</h2>
<p>In addition to allowing users to post stories they write, fanfiction.net allows authors to “favorite” stories they like. Looking at which stories tend to be favorited by the same users gives us a way to understand connections between stories.</p>
<div class="floatrightimgcontainer">
<img src="img/explanation.png" alt="" style="">
<div class="caption">

</div>
</div>
<div class="spaceafterimg">

</div>
<p>In order to analyze this, we must collect a large amount of metadata from fanfiction.net (“scraping”). We note that we don’t actually collect any significant content, just a lot of data about relationships between pieces of content. Fanfiction.net’s terms of service, as the author understands them, allow this with some restrictions:</p>
<blockquote>
<p>4(E) You agree not to use or launch any automated system, including without limitation, “robots,” “spiders,” or “offline readers,” that accesses the Website in a manner that sends more request messages to the FanFiction.Net servers in a given period of time than a human can reasonably produce in the same period by using a conventional on-line web browser. Notwithstanding the foregoing, FanFiction.Net grants the operators of public search engines permission to use spiders to copy materials from the site for the sole purpose of and solely to the extent necessary for creating publicly available searchable indices of the materials, but not caches or archives of such materials…</p>
</blockquote>
<p>In order to ensure compliance with these terms, the author intentionally built significant rate limiting into the scraper and took care to minimize the load put on fanfiction.net. While the issue of academic analysis was not mentioned, it was not excluded and fanfiction.net’s operators have not previously objected to similar academic work. Further, this work could be the preliminary research needed for someone to build a good fanficiton search engine.</p>
<p>Another section of the terms of service prohibits collecting personally identifiable information, which they define to include usernames. As such, I have deliberately discarded all such information and don’t use it. (Though, I note that several search engines do – try searching for an authors name on any major search engine.) I do refer to some usernames in this post, but that was done entirely by hand.</p>
<p>In collecting data, since we are only looking at a subset of users, it is important to be wary of sampling bias. For example, if we sampled authors starting from the favorites of a particular author, or from those who had contributed stories to a community, we might get a very skewed perspective of the stories on fanfiction.net. The author considered a number of approaches, but concluded the fairest approach would be to use the authors of the most reviewed stories on fanfiction.net. This is a bias, but it should bias us towards the most interesting and important parts of the graph.</p>
<h2 id="graph-construction">Graph Construction</h2>
<p>A <a href="http://en.wikipedia.org/wiki/Graph_(mathematics)">graph</a>, in the context of mathematics, is a collection of objects called vertices joined by connections called edges. For example, cities can be thought of as the vertices a graph connected by different highways and roads (the edges).</p>
<div class="centerimgcontainer">
<img src="img/example-graph.svg" alt="" style="">
<div class="caption">
An example of a graph (from Wikipedia)
</div>
</div>
<div class="spaceafterimg">

</div>
<p>A weighted graph is a graph where some edges are “stronger” than others. For example, some cities are connected by giant 6-lane highways, while others are connected by gravel roads. Larger weights represent stronger connections and smaller weights represent weaker ones. A weight of zero is the same thing as having no connection at all.</p>
<p>We will be interpreting fanfiction as a weighted graph, where edges represent a “connection” between stories. We will be using as our weights for edges the probability that someone will like both stories, given that they like one. That is, <span class="math">\(W_{a, b} = \frac{|F_a \cap F_b|}{|F_a \cup F_b|}\)</span> where <span class="math">\(F_s\)</span> is the users who favorited the story <span class="math">\(s\)</span>.</p>
<p>There are lots of other possibilities, some resulting in directed graphs:</p>
<ul>
<li>(directed) The probability that someone who favorites <span class="math">\(a\)</span> will favorite <span class="math">\(b\)</span>: <span class="math">\(W_{a\to b} = \frac{|F_a \cap F_b|}{|F_a|}\)</span></li>
<li>The probability that someone who favorites <span class="math">\(a\)</span> favorites <span class="math">\(b\)</span> times the probability that someone who favorites <span class="math">\(b\)</span> favorites <span class="math">\(a\)</span>: <span class="math">\(W_{a,b} = \frac{|F_a \cap F_b|^2}{|F_a| * |F_b|}\)</span></li>
<li>The lesser of the probability that someone who favorites <span class="math">\(a\)</span> favorites <span class="math">\(b\)</span> and the probability that someone who favorites <span class="math">\(b\)</span> favorites <span class="math">\(a\)</span>: <span class="math">\(W_{a,b} = \min\left(\frac{|F_a \cap F_b|}{|F_a|}, \frac{|F_a \cap F_b|}{|F_b|} \right)\)</span></li>
</ul>
<p>Our experience was that it didn’t matter too much for the results, for large graphs.</p>
<p>(It’s worth noting that many of these could easily generalize to higher-dimensional edges for a weighted hyper-graph.)</p>
<p>In our selected weight definition, <span class="math">\(W_{a, b} = \frac{|F_a \cap F_b|}{|F_a \cup F_b|}\)</span>, we give equal weight to the preferences of all users. But there’s a lot of variance between users: some favorite everything under the sun, while others very selectively favorite stories they really like. If we give the users who favorite thousands of stories the same weight as users who favorite ten, the users who favorite thousands dominate everything (and aren’t a very good signal).</p>
<p>Instead, we give each user <span class="math">\(u\)</span> a weight of <span class="math">\(\frac{1}{20+n(u)}\)</span> where <span class="math">\(n(u)\)</span> denotes the number of stories <span class="math">\(u\)</span> has favorited. This results in a measure on the space of users, <span class="math">\(\mu(S) = \sum_{u \in S} \frac{1}{20+n(u)}\)</span>, and the equation for our weights becomes <span class="math">\(W_{a, b} = \frac{\mu(F_a \cap F_b)}{\mu(F_a \cup F_b)}\)</span>.</p>
<p>Applying these techniques to a couple of the top Harry Potter stories, we get the following graph (using <a href="http://www.graphviz.org/">graphviz</a>):</p>
<div class="bigcenterimgcontainer">
<img src="img/HP-basic.png" alt="Small labeled graph of top Harry Potter stories" style="">
<div class="caption">
Small labeled graph of top Harry Potter stories
</div>
</div>
<div class="spaceafterimg">

</div>
<p>With a small amount of investigation, it’s easy to understand a lot of the graph’s structure. For example, on the lower right hand side, there’s a triangular clique.</p>
<div class="floatrightimgcontainer">
<img src="img/HP-basic-clique.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<p>A quick Google search reveals that this triangular clique consists of the “Dark Prince Trilogy” by Kurinoone. The stories are more strongly linked to their immediate predecessor/successor than the pair separated by a story are to eachother.</p>
<h2 id="large-graph-visualizations-for-harry-potter">Large Graph visualizations for Harry Potter</h2>
<p>If we use different tools, we can visualize much larger graphs.</p>
<p>We consider the top 2,000 most reviewed Harry Potter stories and their authors. Based on the author’s favorite lists, we construct a weighted graph, with the stories as nodes (edge weights are calculated as above).</p>
<p>We then prune the graph’s edges, keeping the top 8,000 most strongly weighted edges. We also prune the nodes, keeping only those with at least one edge. This leaves us with a graph of 1,623 nodes and 8,000 edges.</p>
<p>We then load this graph into the graph visualization tool <a href="https://gephi.org/">gephi</a>. We layout the graph using the OpenOrd and ForceAtlas2 layout algorithms. (OpenOrd was particularly good at extracting clusters. Beyond that, this was largely a matter of aesthetic taste.)</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-HP-blank.png" alt="" style="">
<div class="caption">
Graph of Harry Potter Fanfiction (top 1,623 stories)
</div>
</div>
<div class="spaceafterimg">

</div>
<p>We can see lots of interesting structure in this graph: there are lots of clusters, some more connected than others.</p>
<p>A first hypothesis might be that some of these clusters are caused by language. As it turns out, this is the case:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-HP-lang-labeled.png" alt="" style="">
<div class="caption">
Graph of Harry Potter Fanfiction, colored by language
</div>
</div>
<div class="spaceafterimg">

</div>
<p>Another cause of clusters may be the “ship” (romantic pairing of the story). Many readers have a strong loyalty to a particular ship – for example, they might feel very strongly that Harry and Hermione should be together.</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-HP-ships-labeled.png" alt="" style="">
<div class="caption">
Graph of Harry Potter Fanfiction, colored by ship
</div>
</div>
<div class="spaceafterimg">

</div>
<p>(Note: Ships are inferred from tags story summaries. HP = Harry Potter, HG = Hermione Granger, GW = Ginny Weasley, DM = Draco Malfoy, SS = Severus Snape and LV = Lord Voldemort.)</p>
<p>One interesting point is that by far the most diffused ship is HP/GW. It seems likely that this is because it is the ship we see in cannon Harry Potter, and so many stories not focused on romance default to it and unaligned readers are more tolerant of it.</p>
<p>One striking pattern in fanfiction is that a massive fraction of stories are male/male pairings. Such stories are frequently referred to as “slash.”</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-HP-slash-labeled.png" alt="" style="">
<div class="caption">
Graph of Harry Potter Fanfiction, colored by slash
</div>
</div>
<div class="spaceafterimg">

</div>
<p>Many stories include a slash tag in the summary. Some other stories tag themselves as “no-slash.”</p>
<p>One interesting pattern is that stories tagged “no-slash” concentrate around parts of the border of slash stories. One possible reason may be that authors writing stories that might, from a glance at the summary or characters list, look like slash (for example, a story about Snape mentoring Harry, or Draco and Harry as friends) feel the need to explicitly signal that that is not the topic of their story.</p>
<p>The predisposition of the French cluster towards slash stories is interesting, but the cluster is so small I am hesitant to read anything into it.</p>
<p>You can also explore an <a href="graphs/HP-ship/">interactive graph of Harry Potter fanfiction</a>.</p>
<h2 id="large-graph-visualizations-for-other-fandoms">Large Graph Visualizations for Other Fandoms</h2>
<p>Of course, we can apply the exact same tricks to other fandoms.</p>
<p><strong>Naruto</strong></p>
<p>For example, Naruto is the second biggest fandom. Here’s a graph of it:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-NAR-blank.png" alt="" style="">
<div class="caption">
Graph of top Naruto fanfiction (1,123 nodes and 4,000 edges)
</div>
</div>
<div class="spaceafterimg">

</div>
<p>We can look at languages again:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-NAR-lang-labeled.png" alt="" style="">
<div class="caption">
Graph of top Naruto fanfiction, colored by language
</div>
</div>
<div class="spaceafterimg">

</div>
<p>And also for ships:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-NAR-ships-labeled.png" alt="" style="">
<div class="caption">
Graph of top Naruto fanfiction, colored by ship
</div>
</div>
<div class="spaceafterimg">

</div>
<p><strong>Twilight</strong></p>
<p>And again, we can graph the top twilight stories:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-TWI-blank.png" alt="" style="">
<div class="caption">
Graph of top Twilight fanfiction (1,031 nodes, 5,00 edges)
</div>
</div>
<div class="spaceafterimg">

</div>
<p>We can color it by language:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-TWI-lang-labeled.png" alt="" style="">
<div class="caption">
Graph of top Twilight fanfiction, colored by language
</div>
</div>
<div class="spaceafterimg">

</div>
<p>And by ship:</p>
<div class="bigcenterimgcontainer">
<img src="img/graph-TWI-ships-labeled.png" alt="" style="">
<div class="caption">
Graph of top Twilight fanfiction, colored by ship
</div>
</div>
<div class="spaceafterimg">

</div>
<p>One thing that seems pretty surprising, without inside knowledge of the fandom, is the lack of stories where the pairing involves Jacob. On further inspection, we find that there are stories like that on fanfiction.net, but they aren’t amongst the most highly reviewed. Perhaps this pairing prefers other websites? I’d love comments from anyone with insight into this.</p>
<p>You can also explore an <a href="graphs/NAR-ship/">interactive graph of Naruto fanfiction</a> and of <a href="graphs/TWI-ship/">Twilight fanfiction</a>.</p>
<h2 id="pagerank">PageRank</h2>
<p>What are the best fanfics on fanfiction.net? How can we identify them?</p>
<p>A naive approach would be to select the most favorited or reviewed stories. But people’s quality of taste varies. A more sophisticated approach is Google’s PageRank algorithm which is used to determine which web pages are of high quality.</p>
<p>In a normal vote gives equal weight to every voter. But some voters are better qualified to decide than others. In PageRank, we recalculate the votes again and again, giving each “person’s” vote a weight based on how many votes they received in the previous step.</p>
<p>In the case of the Internet, we interpret a website linking to another website as that website voting for the one it links to. Similarly, we can apply it to fanfiction by interpreting story A as “voting” for a story B with a weight of the probability that a user who likes A also likes B.</p>
<p><strong>Harry Potter top stories by PageRank:</strong></p>
<ol>
        <li>
<a href="http://fanfiction.net/s/1260679">Realizations</a> (16.4)
</li>
        <li>
<a href="http://fanfiction.net/s/2636963">Harry Potter and the Nightmares of Futures Past</a> (15.7)
</li>
        <li>
<a href="http://fanfiction.net/s/2318355">Make A Wish</a> (14.0)
</li>
        <li>
<a href="http://fanfiction.net/s/5554780">Poison Pen</a> (11.7)
</li>
        <li>
<a href="http://fanfiction.net/s/6413108">To Shape and Change</a> (11.5)
</li>
        <li>
<a href="pagerank/hp.html"><b>More</b></a>
</ol>



<p><strong>Naruto top stories by PageRank:</strong></p>
<ol>
        <li>
<a href="http://fanfiction.net/s/2731239">Team 8</a> (11.1)
</li>
        <li>
<a href="http://fanfiction.net/s/6694302">Naruto: Myoushuu no Fuuin</a> (6.42)
</li>
        <li>
<a href="http://fanfiction.net/s/5409165">It’s For a Good Cause, I Swear!</a> (5.57)
</li>
        <li>
<a href="http://fanfiction.net/s/6051938">The Sealed Kunai</a> (5.24)
</li>
        <li>
<a href="http://fanfiction.net/s/3929411">Chunin Exam Day</a> (5.14)
</li>
        <li>
<a href="pagerank/naruto.html"><b>More</b></a>
</ol>


<p><strong>Twilight top stories by PageRank:</strong></p>
<ol>
        <li>
<a href="http://fanfiction.net/s/5100876">The Blessing and the Curse</a> (18.6)
</li>
        <li>
<a href="http://fanfiction.net/s/4901517">Tropic of Virgo</a> (15.0)
</li>
        <li>
<a href="http://fanfiction.net/s/5319052">A Rough Start</a> (12.7)
</li>
        <li>
<a href="http://fanfiction.net/s/4769414">Creature of Habit</a> (12.6)
</li>
        <li>
<a href="http://fanfiction.net/s/6550419">The Plan</a> (10.2)
</li>
        <li>
<a href="pagerank/twi.html"><b>More</b></a>
</ol>

<p>One neat thing we can do is give nodes on our graphs a size based on their PageRank. (We can also color nodes based on the first three components of the singular value decomposition of the adjacency matrix.)</p>
<div class="bigcenterimgcontainer">
<img src="img/HP_union_size_larger.png" alt="" style="">
</div>
<div class="spaceafterimg">

</div>
<h2 id="story-recommendation">Story Recommendation</h2>
<p>There’s something that’s just begging to be done, at this point: story recommendations. Given our knowledge of what stories many users like, can we recommend other stories that they’re probable to like?</p>
<p>This problem is called collaborative filtering, and is a well-established area. Unfortunately, it isn’t something I’m terribly knowledgeable about, so I took a relatively naive approach: sum over the preferences of all users, weighted by how similar their preferences are to the user you are trying to predict.</p>
<p>Specifically, we give each story, <span class="math">\(s\)</span>, a rank <span class="math">\(R_u(s)\)</span>, for a user <span class="math">\(u\)</span>. If the rank is high, we think <span class="math">\(u\)</span> is likely to like <span class="math">\(s\)</span>.</p>
<p><span class="math">\[R_u(s) = \sum_{v\in F_s \setminus \{u\}} \left(\frac{|S(u)\cap S(v)|}{20+|S(v)|}\right)^2\]</span></p>
<p>where <span class="math">\(F_s\)</span> is the set of users who favorited <span class="math">\(s\)</span> and <span class="math">\(S(u)\)</span> is the stories favorited by the user <span class="math">\(u\)</span>.</p>
<p>For example, we can make recommendations for S’TarKan, the author of the most favorited Harry Potter story on fanfiction.net:</p>
<ul>
	<li>
*<a href="http://fanfiction.net/s/2559745">Learning to Breathe</a> (1.459)
</li>
	<li>
*<a href="http://fanfiction.net/s/2954601">Taking Control</a> (1.383)
</li>
	<li>
*<a href="http://fanfiction.net/s/1594791">Backwards Compatible</a> (1.381)
</li>
	<li>
*<a href="http://fanfiction.net/s/2636963">Harry Potter and the Nightmares of Futures Past</a> (1.377)
</li>
	<li>
*<a href="http://fanfiction.net/s/2479927">Harry Potter and Fate’s Debt</a> (1.218)
</li>
	<li>
…
</li>
</ul>

<p>A * denotes that this is already one of the users favorite stories or one of their own stories. We can exclude their favorite stories, and their own stories:</p>
<ul>
	<li>
<a href="http://fanfiction.net/s/2318355">Make A Wish</a> (0.949)
</li>
	<li>
<a href="http://fanfiction.net/s/3401052">A Black Comedy</a> (0.750)
</li>
	<li>
<a href="http://fanfiction.net/s/4536005">Oh God Not Again!</a> (0.679)
</li>
	<li>
<a href="http://fanfiction.net/s/1260679">Realizations</a> (0.642)
</li>
	<li>
<a href="http://fanfiction.net/s/2107570">Lord of Caer Azkaban</a> (0.635)
</li>
	<li>
…
</li>
</ul>

<p>These are all very popular stories. It’s not very useful to S’TarKan if we recommend them extremely popular stories that they’ve almost certainly seen before. As such, it is interesting to penalize the popularity of stories.</p>
<p>Consider <span class="math">\(\frac{R_u(s)}{|F_s|^k}\)</span>. When <span class="math">\(k = 0\)</span>, it’s our original rank. When <span class="math">\(k = 1\)</span>, it full normalizes stories against popularity. And in between, it penalizes popularity to varying degrees. If we set k = 0.7, we get these recommendations:</p>
<ul>
	<li>
<a href="http://fanfiction.net/s/2114122">Insanity</a> (0.034)
</li>
	<li>
<a href="http://fanfiction.net/s/1995612">Shadow of the Serpent</a> (0.032)
</li>
	<li>
<a href="http://fanfiction.net/s/2160456">The Bargain</a> (0.031)
</li>
	<li>
<a href="http://fanfiction.net/s/1975479">Sinners</a> (0.029)
</li>
	<li>
<a href="http://fanfiction.net/s/926568">Harry Potter and the Order of the Phoenix</a> (0.029)
</li>
	<li>
…
</li>
</ul>

<p>You can think of these as stories that are <em>unexpectedly</em> popular amongst similar users. Similar users like them a lot more than random users like them. (Though, perhaps 0.7 is a bit too extreme.)</p>
<p>Curious about what this algorithm would recommend for you? If you’re a popular fanfiction author, you may be in my recommendations for top users for <a href="recs/hp.html">Harry Potter</a>, <a href="recs/nar.html">Naruto</a> or <a href="recs/twi.html">Twilight</a>.</p>
<p>Since my scripts can’t look at your author name while complying with fanfiction.net’s terms of service, you will need to know your <em>author ID</em>. To get it, go to your fanfiction.net profile page and look at the URL. It will be of the form: <code>http://fanfiction.net/u/author_ID/...</code>. Then search for your author ID in the file!</p>
<p>I’m certain one could do much better if they wanted to put a bit more effort into it. :)</p>
<h2 id="conclusion">Conclusion</h2>
<p>In light of all this, I’d like to reflect on a few things.</p>
<p><strong>Big Data</strong>: A year ago, I was very dismissive of “big data” as a buzzword. Primarily, it seems to be thrown around by business people who don’t really understand much. But one thing I’ve learned in explorations of data like this one and working in machine learning, is that there is something very powerful about larger amounts of data. There’s something very qualitatively different. The fanfiction data I used was actually quite small, only a few hundred users, because of how I limited the amount I downloaded, but I think it still demonstrates the sorts of things that become possible as you have larger amounts of data. (To be honest, a much more compelling example is the progress that’s been made in computer vision using ImageNet… But this still influenced my views.)</p>
<p><strong>Digital Humanities</strong>: Digital humanities also seems to be a bit of a buzzword. But I hope this provides a simple example of the power that can come from applying a little bit of math and computer science to humanities problems.</p>
<p><strong>Metadata and Privacy</strong>: In this essay, we analyzed stories by looking at whether they were favorited by the same users. There’s a natural “dual” to this: analyzing users by looking at whether they favorited the same stories. This would give us a graph of connections between users and allow us to find clusters of users. But what if you use other forms of metadata? For example, we now know that the US government has metadata on who phones who. It seems very likely that many companies and governments have information on where your cellphone is as a function of time. All this can construct a graph of society. I can’t really fathom how much one must be able to learn about someone from that. (And how easy it would be to misinterpret.)</p>
<p><strong>Fanfiction Websites</strong>: I think there’s a lot of potential for fanfiction websites to better serve their users based on the techniques outlined here. I’d be really thrilled to see fanfiction.net or Archive Of Our Own adopt some of these ideas. Imagine being able to list a handful of stories in some category you’re interested in and discover others? Or get good recommendations? The ideas are all pretty straightforward once you think of them. I’d be very happy to talk to the groups behind different fanfiction websites and provide some help or share example code.</p>
<p><strong>Deep Learning and NLP</strong>: Recently, there’s been some really cool results in applying Deep Learning to Natural Language Processing. One would need a lot more data than I collected, and it would take more effort, but I bet one could do some really interesting things here.</p>
<p><strong>t-SNE</strong>: <a href="http://homepage.tudelft.nl/19j49/t-SNE.html">t-Distributed Stochastic Neighbor Embedding</a>, is an algorithm for visualizing the structure of high-dimensional data. It would be a much simpler approach to understanding the structure of fanfiction than the graph based one I used here, and probably give much better results. If I was starting again, I would use it.</p>
<p><strong>Resources</strong>: In principle, I’d really like to share my code and make it easy for people to replicate the work I described here. However, I think that would be really rude to fanfiction.net because it could result in lots of people scraping their website, and it seems likely many would remove my rate limiter. An alternative would be to share my extracted metadata, but, again, I think it would be really rude to do that without fanfiction.net’s permission, and possibly a violation of their terms of service. So, in the end, I’m not sharing any resources. That said, all of this can be done pretty easily.</p>
<p><em>(This post is a fun experiment done primarily for amusement. I would be delighted to hear your comments and thoughts: you can comment inline or at the end. For typos, technical errors, or clarifications you would like to see added, you are encouraged to make a pull request on <a href="https://github.com/colah/Fanfiction-Graphs-Post">github</a>. If you enjoyed this post, you might consider subscribing to my <a href="../../rss.xml">RSS feed</a>.)</em></p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thank you to Eliana Lorch, Taren Stinebrickner-Kauffman, Mary Becica, and Jacob Steinhardt for their comments and encouragement.</p>]]></description>
    <pubDate>Sun, 06 Jul 2014 00:00:00 UT</pubDate>
    <guid>http://colah.github.io/posts/2014-07-FFN-Graphs-Vis/</guid>
</item>

    </channel> 
</rss>
