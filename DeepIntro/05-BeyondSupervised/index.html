<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <title>Unsupervised Learning & Autoencoders</title>

    <meta name="description" content="This talk tries to get at the deep questions of deep learning: What is the problem we are really trying to solve? What is interesting about deep models?">
    <meta name="author" content="Chris Olah">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/simple.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h2>Beyond Supervised Learning</h2>
          <br>
          <h3>DeepIntro</h3>
        </section>

        <section>
          <h2>Some Terminology</h2>
          <br>
          <p><b>Labeled Data</b>: Data annotated with tags.<br> For example, images with classes.</p>
        </section>


        <section>
          <h2>Some Terminology</h2>
          <br>
          <p><b>Supervised Learning</b><br></p>
        </section>

        <section>
          <h2>Some Terminology</h2>
          <br>
          <p><b>Unsupervised Learning</b><br></p>
          <p>What metric?</p>
        </section>

        <section>
          <h2>Some Terminology</h2>
          <br>
          <p><b>Semi-supervised Learning</b><br></p>
        </section>

        <section>
          <h2>Some Terminology</h2>
          <br>
          <p><b>Generative Models</b><br></p>
        </section>

        <section>
          <h2>Some Terminology</h2>
          <br>
          <p><b>Active Learning</b><br></p>
        </section>

        <section>
          <h2>What Neural Nets are Good at</h2>
          <br>
          <p><img src="imgs/nn-map.png"></p>
        </section>

        <section>
          <h2>But what if we only have Xs?</h2>
          <br>
          <h2>Can we still learn about X?</h2>
        </section>

        <section>
          <h2>One natural thing...</h2>
          <br>
          <p><img src="imgs/nn-map-xx.png"></p>
        </section>

        <section>
          <h1>Autoencoders</h1>
        </section>

        <section>
          <p><img src="imgs/Autoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/AutoencoderManifold.png"></p>
        </section>

        <section>
          <h2>MNIST compressed through 30 dimensions</h2>
          <br>
          <p><img src="imgs/mnist-auto-hinton.png"></p>
          <br>
          <p>Hinton & Salakhutdinov (2006)</p>
        </section>

        <section>
          <p><img src="imgs/DeepAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/StackedAutoencoder.png"></p>
        </section>

        <section>
          <h2>"Smaller"? ...</h2>
        </section>

        <section>
          <p><img src="imgs/OvercompleteAutoencoder.png"></p>
        </section>

        <section>
          <p>The real question is: What do we want from a representation?</p>
          <br>
          <p>(No one knows.)</p>
        </section>

        <section>
          <h2>Some flavor of the possibilities...</h2>
        </section>

        <section>
          <p><img src="imgs/OvercompleteAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/SparseAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/SparseAutoencoderManifold.png"></p>
        </section>

        <section>
          <p><img src="imgs/ContractiveAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/ContractiveAutoencoderManifold.png"></p>
        </section>

        <section>
          <p><img src="imgs/DenoisingAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/DenoisingAutoencoderManifold.png"></p>
        </section>

        <section>
          <p>(More on denoising autoencoders in a bit!)</p>
        </section>

        <section>
          <h2>More Kinds of Autoencoders</h2>
        </section>

        <section>
          <p><img src="imgs/ConvolutionalAutoencoder.png"></p>
          <p>(Sometimes add pooling.)</p>
        </section>

        <section>
          <p><img src="imgs/LocallyConnectedAutoencoder.png"></p>
        </section>

        <section>
          <h2>"Cat Brain" (Le, <i>et al.</i> (2012))</h2>
          <p><img src="imgs/FaceDetector.png"></p>
          <p>Locally connected, pooling, conrast normalization, etc, trained on Youtube. Big.</p>
        </section>

        <section>
          <h2>"Cat Brain" (Le, <i>et al.</i> (2012))</h2>
          <p><img src="imgs/LeNeurons.png"></p>
          <p>Locally connected, pooling, conrast normalization, etc, trained on Youtube. Big.</p>
        </section>

        <section>
          <h2>Pre-Training</h2>
        </section>

        <section>
          <p><img src="imgs/PreTraining.png"></p>
        </section>

        <section>
          <p><img src="imgs/PretrainingVis.png"></p>
          <p>Erhan, <i>et al.</i> (2010)</p>
        </section>

        <section>
          <h2>Generative Models</h2>
        </section>

        <section>
          <h2>Generative Models</h2>
          <br>
          <p><b>Generative Model</b>: \(~p(x)\)<br> For example, generating pictures.<br><br></p>
          <p><b>Discriminative Model</b>: \(~p(y|x)\)<br> For example, classifying pictures.<br><br></p>
        </section>

        <section>
          <h2>Generative Models</h2>
          <br>
          <p><b>Sampling</b>:<br>Give us an image.<br><br></p>
          <p><b>Probability Density</b>:<br>Is this a reasonable image?<br><br></p>
          <p><b>Equivalent.</b></p>
        </section>

        <section>
          <h2>Neural Net Generative Models are Hard</h2>
        </section>

        <section>
          <h3>We can reframe the problem</h3><br>
          <p>"The cat sat..." predict the next word.</p><br>
          <h3>But often doesn't work very well.</h3>
        </section>

        <section>
          <h3>Autoregressive models</h3><br>
          <p>NADE</p><br>
          <p>RNNs</p>
        </section>

        <section>
          <h2>Other ideas?</h2><br>
        </section>

        <section>
          <h2>Maybe...</h2><br>
          <p><img src="imgs/nn-map-x1.png"></p>
          <br>
          <p>(Neural net: Everything is a 1!!!!!!!!)</p>
        </section>

        <section>
          <h2>Maybe Negative sampling?</h2><br>
          <p><img src="imgs/nn-map-x10.png"></p>
          <br>
          <p>(Hard counterexamples are hard; not actually generative.)
        </section>

        <section>
          <h2>Fundamental Problem: Making things sum to one.</h2><br>
          <p>"Partition Function"</p>
        </section>

        <section>
          <h2>But some things do work!</h2>
        </section>

        <section>
          <h2>Denoising Autoencoders</h2>
        </section>

        <section>
          <p><img src="imgs/DenoisingAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/DenoisingAutoencoderManifold.png"></p>
        </section>

        <section>
          <h2>Denoising Autoencoders Can Be Sampled</h2>
        </section>


        <section>
          <p><img src="imgs/DenoisingSamping.png"></p>
          <p><img src="imgs/DenoisingSamples.png"></p>
          <p>(Samples from Bengio, <i>et al.</i> (2013))</p>
        </section>


        <section>
          <h2>Generative Adverserial Networks</h2>
        </section>

        <section>
          <p><img src="imgs/GAN.png"></p>
        </section>

        <section>
          <p><img src="imgs/AdverserialSamples.png"></p>
          <p>Goodfellow, <i>et al.</i> (2014)</p>
        </section>

        <section>
          <p><img src="imgs/AdverserialDistributions.png"></p>
          <p>Goodfellow, <i>et al.</i> (2014)</p>
        </section>

        <section>
          <p><img src="imgs/dcgan.png"></p>
          <p>Radford, <i>et al.</i> (2015)</p>
        </section>

        <section>
          <h2>InfoGANs</h2>
        </section>

        <section>
          <p><img src="imgs/InfoGAN1.png" style="width:55%"></p>
        </section>

        <section>
          <p><img src="imgs/InfoGAN2.png" style="width:55%"></p>
        </section>

        <section>
          <p><img src="imgs/InfoGANresult.png"></p>
        </section>

        <section>
          <h2>Variational Autoencoders</h2>
        </section>

        <section>
          <p><img src="imgs/VariationalAutoencoder.png"></p>
        </section>

        <section>
          <p><img src="imgs/VA-MNIST-samples.png"></p>
          <p>Kingma & Welling (2013)</p>
        </section>

        <section>
          <h2>Semi-Supervised Learning</h2>
        </section>

        <section>
          <h2>Ladder Nets</h2>
          <p>Rasmus et al, 2015</p>
          <br>
        </section>

        <section>
          <h2>Auxilliary Generative Models</h2>
          <p>Maal√∏e et al, 2016</p>
          <br>
        </section>

        <section>
          <h2>Improved Techniques for Training GANs</h2>
          <p>Salimans et al, 2016</p>
          <br>
        </section>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: 'none', //Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

        // Parallax scrolling
        // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
        // parallaxBackgroundSize: '2100px 900px',

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
      });

    </script>

  </body>
</html>
